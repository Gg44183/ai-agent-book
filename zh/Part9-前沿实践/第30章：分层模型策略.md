# ç¬¬ 30 ç« ï¼šåˆ†å±‚æ¨¡å‹ç­–ç•¥

> **"åˆ†å±‚è·¯ç”±èƒ½èŠ‚çœ 80% æˆæœ¬"â€”â€”ä½†å‰ææ˜¯ä½ èƒ½æ­£ç¡®åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦ï¼Œè¿™æ¯”å¬èµ·æ¥éš¾å¾—å¤šã€‚**

---

> **â±ï¸ å¿«é€Ÿé€šé“**ï¼ˆ5 åˆ†é’ŸæŒæ¡æ ¸å¿ƒï¼‰
>
> 1. ä¸‰å±‚æ¨¡å‹ï¼šSmallï¼ˆ50%æµé‡ï¼‰â†’ Mediumï¼ˆ40%ï¼‰â†’ Largeï¼ˆ10%ï¼‰
> 2. å¤æ‚åº¦åˆ¤æ–­ï¼šè§„åˆ™ä¼˜å…ˆï¼ˆå…³é”®è¯/é•¿åº¦ï¼‰ï¼ŒLLM å…œåº•ï¼ˆæ¨¡ç³Šä»»åŠ¡ï¼‰
> 3. å‡çº§æœºåˆ¶ï¼šSmall è¾“å‡ºè´¨é‡å·®æ—¶è‡ªåŠ¨å‡çº§åˆ° Medium/Large
> 4. æˆæœ¬ç›‘æ§ï¼šæŒ‰å±‚ç»Ÿè®¡ Token æ¶ˆè€—ï¼ŒLarge å æ¯”è¶… 20% è¦æ’æŸ¥
> 5. ç¼“å­˜ä¼˜å…ˆï¼šç›¸åŒ Query å‘½ä¸­ç¼“å­˜ï¼Œæ¯”æ¨¡å‹åˆ†å±‚æ›´çœé’±
>
> **10 åˆ†é’Ÿè·¯å¾„**ï¼š30.1-30.3 â†’ 30.5 â†’ Shannon Lab

---

## 30.1 ä»ä¸€ä¸ªè´¦å•è¯´èµ·

ä½ çš„ AI Agent ç³»ç»Ÿä¸Šçº¿ä¸‰ä¸ªæœˆï¼Œè€æ¿æ‰¾ä½ è°ˆè¯ï¼š

"è¿™ä¸ªæœˆ LLM è´¦å• $15,000ï¼Œæ¯”é¢„ç®—é«˜äº† 10 å€ã€‚"

ä½ æ‹‰å‡ºä½¿ç”¨æ—¥å¿—ï¼Œå‘ç°é—®é¢˜ï¼š

```
[2025-01-10 09:15:22] ç”¨æˆ·é—®: "ä»Šå¤©å‘¨å‡ ?"
                       æ¨¡å‹: claude-opus-4-1
                       æ¶ˆè€—: 0.12 USD

[2025-01-10 09:15:45] ç”¨æˆ·é—®: "å¸®æˆ‘åˆ†æè¿™ä»½ 200 é¡µè´¢æŠ¥çš„æ ¸å¿ƒé£é™©ç‚¹"
                       æ¨¡å‹: claude-opus-4-1
                       æ¶ˆè€—: 0.35 USD
```

ä¸¤ä¸ªä»»åŠ¡ç”¨äº†åŒæ ·æ˜‚è´µçš„é¡¶çº§æ¨¡å‹ã€‚é—®"ä»Šå¤©å‘¨å‡ "èŠ±äº† $0.12â€”â€”è¿™ä¸ªä»·æ ¼ï¼ŒHaiku èƒ½å›ç­” 200 æ¬¡ã€‚

**é—®é¢˜çš„æœ¬è´¨**ï¼šä½ çš„ç³»ç»Ÿåœ¨"è¿‡åº¦æœåŠ¡"â€”â€”ç”¨ Rolls-Royce æ¥é©¾é€èœï¼Œç”¨ç«ç®­ç‚®æ‰“èšŠå­ã€‚

è¿™ç« æˆ‘ä»¬è§£å†³ä¸€ä¸ªé—®é¢˜ï¼š**å¦‚ä½•è®©å¯¹çš„ä»»åŠ¡ç”¨å¯¹çš„æ¨¡å‹**ã€‚

---

## 30.2 ä¸‰å±‚æ¨¡å‹æ¶æ„

### ä¸ºä»€ä¹ˆæ˜¯ä¸‰å±‚ï¼Ÿ

| å±‚çº§ | é€‚ç”¨åœºæ™¯ | ä»£è¡¨æ¨¡å‹ | ç›¸å¯¹æˆæœ¬ |
|------|----------|----------|----------|
| **Small** | ç®€å•é—®ç­”ã€æ ¼å¼åŒ–ã€åˆ†ç±» | claude-haiku, gpt-5-nano | 1x |
| **Medium** | æ ‡å‡†åˆ†æã€æ€»ç»“ã€ä»£ç è¾…åŠ© | claude-sonnet, gpt-5-mini | 3-5x |
| **Large** | å¤æ‚æ¨ç†ã€åˆ›æ„å†™ä½œã€æ·±åº¦åˆ†æ | claude-opus, gpt-5.1 | 50-150x |

ä¸‰å±‚çš„è®¾è®¡æ¥è‡ªå®è·µè§‚å¯Ÿâ€”â€”å¤§å¤šæ•°ç”Ÿäº§æµé‡å¯ä»¥å½’ç±»åˆ°è¿™ä¸‰ä¸ªæ¡¶é‡Œï¼š

![ä¸‰å±‚æ¨¡å‹ä»»åŠ¡åˆ†å¸ƒ](assets/model-tiers-distribution.svg)

**ç›®æ ‡åˆ†å¸ƒ**ï¼š50% Small / 40% Medium / 10% Largeã€‚

è¿™ä¸æ˜¯å¼ºåˆ¶é…é¢â€”â€”æ˜¯å‚è€ƒåŸºå‡†ã€‚å®é™…åˆ†å¸ƒå–å†³äºä½ çš„ä¸šåŠ¡åœºæ™¯ã€‚ä½†å¦‚æœä½ çš„ Large å æ¯”è¶…è¿‡ 20%ï¼Œè¯´æ˜å¤æ‚åº¦åˆ¤æ–­å¯èƒ½æœ‰é—®é¢˜ã€‚

### é…ç½®ç»“æ„

ğŸ“¦ **å®ç°å‚è€ƒ (Shannon)**: [`config/models.yaml`](https://github.com/Kocoro-lab/Shannon/blob/main/config/models.yaml) - model_tiers é…ç½®

```yaml
# Shannon ä¸‰å±‚æ¨¡å‹é…ç½®
model_tiers:
  small:
    # ç›®æ ‡å æ¯”: 50% - å¿«é€Ÿä½æˆæœ¬ï¼Œå¤„ç†åŸºç¡€ä»»åŠ¡
    providers:
      - provider: anthropic
        model: claude-haiku-4-5-20251015
        priority: 1  # é¦–é€‰
      - provider: openai
        model: gpt-5-nano-2025-08-07
        priority: 2  # å¤‡é€‰
      - provider: xai
        model: grok-3-mini
        priority: 3
      - provider: google
        model: gemini-2.5-flash-lite
        priority: 4

  medium:
    # ç›®æ ‡å æ¯”: 40% - èƒ½åŠ›/æˆæœ¬å¹³è¡¡
    providers:
      - provider: anthropic
        model: claude-sonnet-4-5-20250929
        priority: 1
      - provider: openai
        model: gpt-5-mini-2025-08-07
        priority: 2
      - provider: xai
        model: grok-4-fast-non-reasoning
        priority: 3
      - provider: google
        model: gemini-2.5-flash
        priority: 4

  large:
    # ç›®æ ‡å æ¯”: 10% - æ·±åº¦æ¨ç†ä»»åŠ¡
    providers:
      - provider: openai
        model: gpt-5.1
        priority: 1
      - provider: anthropic
        model: claude-opus-4-1-20250805
        priority: 2
      - provider: xai
        model: grok-4-fast-reasoning
        priority: 3
      - provider: google
        model: gemini-2.5-pro
        priority: 4
```

**ä¼˜å…ˆçº§è¯´æ˜**ï¼šæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ã€‚åŒå±‚çº§å†…æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•â€”â€”é¦–é€‰ä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡é€‰ã€‚

---

## 30.3 æˆæœ¬ä¸èƒ½åŠ›çš„æ•°å­¦

### å®é™…å®šä»·å¯¹æ¯”

> âš ï¸ **æ—¶æ•ˆæ€§æç¤º** (2026-01): æ¨¡å‹å®šä»·å’Œèƒ½åŠ›åˆ—è¡¨é¢‘ç¹å˜åŒ–ã€‚ä»¥ä¸‹ä¸ºç¤ºæ„é…ç½®ï¼Œè¯·æŸ¥é˜…å„å‚å•†å®˜ç½‘è·å–æœ€æ–°ä»·æ ¼ï¼š[OpenAI Pricing](https://openai.com/pricing) | [Anthropic Pricing](https://www.anthropic.com/pricing) | [Google AI Pricing](https://ai.google.dev/pricing)

| æ¨¡å‹ | Input/1K tokens | Output/1K tokens | ç›¸å¯¹æˆæœ¬ |
|------|-----------------|------------------|----------|
| claude-haiku-4-5 | $0.0001 | $0.0005 | 1x |
| gpt-5-nano | $0.00005 | $0.0004 | ~0.75x |
| claude-sonnet-4-5 | $0.0003 | $0.0015 | 3x |
| gpt-5-mini | $0.00025 | $0.002 | 3.5x |
| claude-opus-4-1 | $0.015 | $0.075 | 150x |
| gpt-5.1 | $0.00125 | $0.01 | 20x |

**å…³é”®æ´å¯Ÿ**ï¼šOpus çš„å•æ¬¡è°ƒç”¨æˆæœ¬æ˜¯ Haiku çš„ **150 å€**ã€‚

### æˆæœ¬èŠ‚çœè®¡ç®—

å‡è®¾æ¯æœˆ 100 ä¸‡æ¬¡ API è°ƒç”¨ï¼Œå¹³å‡æ¯æ¬¡ 1000 tokensï¼š

**åœºæ™¯ Aï¼šå…¨ç”¨ Large æ¨¡å‹**
```
1M * 1K tokens * ($0.015 + $0.075) / 1K = $90,000/æœˆ
```

**åœºæ™¯ Bï¼šæ™ºèƒ½åˆ†å±‚ (50/40/10)**
```
Small:  500K * 1K * ($0.0001 + $0.0005) / 1K = $300
Medium: 400K * 1K * ($0.0003 + $0.0015) / 1K = $720
Large:  100K * 1K * ($0.015 + $0.075) / 1K   = $9,000
æ€»è®¡: $10,020/æœˆ
```

**èŠ‚çœ**: $90,000 - $10,020 = **$79,980/æœˆ (89% é™ä½)**

è¿™å°±æ˜¯åˆ†å±‚ç­–ç•¥çš„å¨åŠ›ã€‚ä½†å‰ææ˜¯â€”â€”ä½ å¾—èƒ½å‡†ç¡®åˆ¤æ–­å“ªäº›ä»»åŠ¡è¯¥ç”¨å“ªä¸ªå±‚çº§ã€‚

---

## 30.4 å¤æ‚åº¦åˆ†æï¼šè·¯ç”±çš„æ ¸å¿ƒ

### å¤æ‚åº¦é˜ˆå€¼é…ç½®

```yaml
# å¤æ‚åº¦â†’å±‚çº§æ˜ å°„
workflows:
  complexity:
    simple_threshold: 0.3   # complexity < 0.3 â†’ small
    medium_threshold: 0.5   # 0.3 â‰¤ complexity < 0.5 â†’ medium
                            # complexity â‰¥ 0.5 â†’ large
```

### å¯å‘å¼å¤æ‚åº¦è®¡ç®—

ğŸ“¦ **å®ç°å‚è€ƒ (Shannon)**: [`llm_service/api/complexity.py`](https://github.com/Kocoro-lab/Shannon/blob/main/python/llm-service/llm_service/api/complexity.py) - _heuristic_analysis å‡½æ•°

```python
def calculate_complexity(query: str, context: Dict) -> float:
    """
    è®¡ç®—ä»»åŠ¡å¤æ‚åº¦åˆ†æ•° (0.0 - 1.0)

    å¯å‘å¼è§„åˆ™ï¼Œä¸ä¾èµ– LLM è°ƒç”¨â€”â€”ç”¨äºå¿«é€Ÿè·¯ç”±å†³ç­–ã€‚
    """
    score = 0.0
    query_lower = query.lower()

    # 1. æŸ¥è¯¢é•¿åº¦ï¼ˆé•¿æŸ¥è¯¢é€šå¸¸æ›´å¤æ‚ï¼‰
    word_count = len(query.split())
    if word_count > 100:
        score += 0.2
    elif word_count > 50:
        score += 0.1

    # 2. å…³é”®è¯æ£€æµ‹
    complex_keywords = [
        "analyze", "compare", "evaluate", "synthesize",
        "design", "architect", "optimize", "debug",
        "explain why", "trade-offs", "implications",
    ]
    simple_keywords = [
        "what is", "define", "list", "format",
        "convert", "translate", "summarize",
    ]

    for kw in complex_keywords:
        if kw in query_lower:
            score += 0.15

    for kw in simple_keywords:
        if kw in query_lower:
            score -= 0.1

    # 3. ä¸Šä¸‹æ–‡ä¿¡æ¯
    if context.get("requires_reasoning"):
        score += 0.3
    if context.get("requires_code_generation"):
        score += 0.2
    if context.get("multi_step"):
        score += 0.2
    if context.get("available_tools") and len(context["available_tools"]) > 5:
        score += 0.1  # å¤šå·¥å…·åœºæ™¯é€šå¸¸æ›´å¤æ‚

    return max(0.0, min(1.0, score))


def select_tier(complexity: float, config: Dict) -> str:
    """æ ¹æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å‹å±‚çº§"""
    simple_threshold = config.get("simple_threshold", 0.3)
    medium_threshold = config.get("medium_threshold", 0.5)

    if complexity < simple_threshold:
        return "small"
    elif complexity < medium_threshold:
        return "medium"
    else:
        return "large"
```

### åŸºäºæ¨¡å‹çš„å¤æ‚åº¦åˆ†æ

å¯å‘å¼è§„åˆ™å¿«ä½†ä¸å¤Ÿå‡†ç¡®ã€‚å¯¹äºé‡è¦å†³ç­–ï¼Œå¯ä»¥ç”¨å°æ¨¡å‹å…ˆåšå¤æ‚åº¦åˆ¤æ–­ï¼š

```python
async def model_based_complexity(query: str, providers) -> Dict:
    """ç”¨å°æ¨¡å‹åˆ†æå¤æ‚åº¦ï¼Œæˆæœ¬çº¦ $0.0001"""

    sys_prompt = (
        "You classify tasks into simple, standard, or complex. "
        "IMPORTANT: Tasks requiring calculations or tool usage "
        "must be 'standard' mode (not 'simple'). "
        "Simple mode is ONLY for direct Q&A without tools. "
        'Respond with JSON: {"recommended_mode": ..., '
        '"complexity_score": 0..1, "reasoning": ...}'
    )

    result = await providers.generate_completion(
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": f"Query: {query}"},
        ],
        tier=ModelTier.SMALL,  # ç”¨å°æ¨¡å‹åˆ¤æ–­
        max_tokens=256,
        temperature=0.0,
        response_format={"type": "json_object"},
    )

    return json.loads(result.get("output_text", "{}"))
```

**æˆæœ¬æƒè¡¡**ï¼šæ¨¡å‹åˆ¤æ–­æ›´å‡†ç¡®ï¼Œä½†æ¯æ¬¡å¤šèŠ± ~$0.0001ã€‚å¯¹äº Large æ¨¡å‹ï¼ˆ$0.09/æ¬¡ï¼‰æ¥è¯´ï¼Œé¿å…ä¸€æ¬¡è¯¯åˆ¤å°±èƒ½è¦†ç›– 900 æ¬¡åˆ¤æ–­æˆæœ¬ã€‚

---

## 30.5 LLM Managerï¼šç»Ÿä¸€è·¯ç”±å±‚

### æ ¸å¿ƒæ¶æ„

ğŸ“¦ **å®ç°å‚è€ƒ (Shannon)**: [`llm_provider/manager.py`](https://github.com/Kocoro-lab/Shannon/blob/main/python/llm-service/llm_provider/manager.py) - LLMManager ç±»

```python
class LLMManager:
    """
    ç»Ÿä¸€ LLM ç®¡ç†ï¼š
    - Provider æ³¨å†Œå’Œè·¯ç”±
    - æ¨¡å‹åˆ†å±‚å’Œé€‰æ‹©
    - ç¼“å­˜å’Œé™æµ
    - Token é¢„ç®—æ§åˆ¶
    - ä½¿ç”¨é‡è¿½è¸ª
    """

    def __init__(self, config_path: Optional[str] = None):
        self.registry = LLMProviderRegistry()
        self.cache = CacheManager(max_size=1000)
        self.rate_limiters: Dict[str, RateLimiter] = {}
        self._breakers: Dict[str, _CircuitBreaker] = {}

        # Token ä½¿ç”¨é‡è¿½è¸ª
        self.session_usage: Dict[str, TokenUsage] = {}
        self.task_usage: Dict[str, TokenUsage] = {}

        # Tier è·¯ç”±åå¥½
        self.tier_preferences: Dict[str, List[str]] = {}

        # åŠ è½½é…ç½®
        if config_path:
            self.load_config(config_path)
```

### Provider é€‰æ‹©é€»è¾‘

```python
def _select_provider(self, request: CompletionRequest) -> tuple[str, LLMProvider]:
    """ä¸ºè¯·æ±‚é€‰æ‹©æœ€ä½³ provider"""

    # 1. æ˜¾å¼æŒ‡å®š providerï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    if request.provider_override:
        provider_name = request.provider_override
        if provider_name not in self.registry.providers:
            raise ValueError(f"Invalid provider_override: {provider_name}")
        if self._is_breaker_open(provider_name):
            raise RuntimeError(f"Provider '{provider_name}' circuit breaker is open")
        return provider_name, self.registry.providers[provider_name]

    # 2. æ˜¾å¼æŒ‡å®šæ¨¡å‹ï¼Œæ‰¾å¯¹åº” provider
    if request.model:
        for pname, pprovider in self.registry.providers.items():
            if self._is_breaker_open(pname):
                continue
            if request.model in pprovider.models:
                return pname, pprovider
        # æ‰¾ä¸åˆ°å°±æ¸…ç©ºï¼Œå›é€€åˆ°å±‚çº§é€‰æ‹©
        request.model = None

    # 3. æŒ‰å±‚çº§åå¥½é€‰æ‹©
    tier_prefs = self.tier_preferences.get(request.model_tier.value, [])

    for pref in tier_prefs:
        if ":" in pref:
            provider_name, model_id = pref.split(":", 1)
            if provider_name in self.registry.providers:
                if self._is_breaker_open(provider_name):
                    continue
                provider = self.registry.providers[provider_name]
                if model_id in provider.models:
                    request.model = model_id  # é”å®šæ¨¡å‹
                    return provider_name, provider

    # 4. å›é€€åˆ° registry é»˜è®¤é€‰æ‹©
    return self.registry.select_provider_for_request(request)
```

**è®¾è®¡è¦ç‚¹**ï¼š
1. **Override æœ€ä¼˜å…ˆ**ï¼šå…è®¸è°ƒç”¨æ–¹å¼ºåˆ¶æŒ‡å®š provider/model
2. **Circuit Breaker æ„ŸçŸ¥**ï¼šè·³è¿‡ä¸å¥åº·çš„ provider
3. **å±‚çº§è·¯ç”±**ï¼šæŒ‰ä¼˜å…ˆçº§å°è¯•åŒå±‚çº§å†…çš„å¤šä¸ªé€‰é¡¹
4. **ä¼˜é›…é™çº§**ï¼šæ‰¾ä¸åˆ°å®Œç¾åŒ¹é…æ—¶æœ‰ fallback

---

## 30.6 Fallback ä¸ç†”æ–­

### Circuit Breaker æ¨¡å¼

å½“æŸä¸ª provider è¿ç»­å¤±è´¥ï¼Œè‡ªåŠ¨ç†”æ–­å¹¶åˆ‡æ¢åˆ°å¤‡é€‰ï¼š

```python
class _CircuitBreaker:
    """
    çŠ¶æ€æœºï¼šclosed â†’ open â†’ half-open â†’ closed

    - closed: æ­£å¸¸å·¥ä½œï¼Œè®°å½•å¤±è´¥æ¬¡æ•°
    - open: ç†”æ–­çŠ¶æ€ï¼Œæ‹’ç»æ‰€æœ‰è¯·æ±‚
    - half-open: å†·å´åå…è®¸æ¢æµ‹è¯·æ±‚
    """

    def __init__(
        self,
        name: str,
        failure_threshold: int = 5,      # è¿ç»­å¤±è´¥ N æ¬¡è§¦å‘ç†”æ–­
        recovery_timeout: float = 60.0,  # ç†”æ–­å†·å´æ—¶é—´ï¼ˆç§’ï¼‰
    ):
        self.name = name
        self.failure_threshold = max(1, failure_threshold)
        self.recovery_timeout = recovery_timeout
        self.failures = 0
        self.state = "closed"
        self.opened_at = 0.0

    def allow(self) -> bool:
        if self.state == "closed":
            return True
        if self.state == "open":
            # å†·å´åè¿›å…¥ half-openï¼ŒåŠ å…¥æŠ–åŠ¨é¿å…æƒŠç¾¤
            jitter = self.recovery_timeout * random.uniform(-0.1, 0.1)
            if (time.time() - self.opened_at) >= (self.recovery_timeout + jitter):
                self.state = "half-open"
                return True  # å…è®¸ä¸€æ¬¡æ¢æµ‹
            return False
        return True  # half-open å…è®¸æ¢æµ‹

    def on_success(self):
        if self.state in ("open", "half-open"):
            self._close()
        self.failures = 0

    def on_failure(self, transient: bool):
        if not transient:
            return  # éç¬æ€é”™è¯¯ä¸è®¡å…¥
        self.failures += 1
        if self.failures >= self.failure_threshold and self.state != "open":
            self._open()
```

### Fallback é€‰æ‹©

```python
def _get_fallback_provider(
    self, failed_provider: str, tier: ModelTier
) -> Optional[tuple[str, LLMProvider]]:
    """ä¸» provider å¤±è´¥æ—¶é€‰æ‹©å¤‡é€‰"""

    tier_prefs = self.tier_preferences.get(tier.value, [])

    for pref in tier_prefs:
        provider_name = pref.split(":")[0] if ":" in pref else pref
        if (
            provider_name != failed_provider
            and provider_name in self.registry.providers
            and not self._is_breaker_open(provider_name)
        ):
            return provider_name, self.registry.providers[provider_name]

    return None  # æ²¡æœ‰å¯ç”¨å¤‡é€‰
```

**å…³é”®è®¾è®¡**ï¼š
- åŒå±‚çº§å†…æŒ‰ä¼˜å…ˆçº§é€‰æ‹©å¤‡é€‰
- è·³è¿‡å·²ç†”æ–­çš„ provider
- è¿”å› None è®©ä¸Šå±‚å†³å®šæ˜¯å¦é™çº§åˆ°å…¶ä»–å±‚

---

## 30.7 æ¨¡å‹èƒ½åŠ›çŸ©é˜µ

ä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½èƒ½åšæ‰€æœ‰äº‹ã€‚æœ‰äº›ä»»åŠ¡éœ€è¦è§†è§‰ç†è§£ï¼Œæœ‰äº›éœ€è¦æ·±åº¦æ¨ç†ã€‚

### èƒ½åŠ›æ ‡è®°

ğŸ“¦ **å®ç°å‚è€ƒ (Shannon)**: [`config/models.yaml`](https://github.com/Kocoro-lab/Shannon/blob/main/config/models.yaml) - model_capabilities é…ç½®

```yaml
model_capabilities:
  # æ”¯æŒå›¾ç‰‡è¾“å…¥çš„æ¨¡å‹
  multimodal_models:
    - gpt-5.1
    - gpt-5-pro-2025-08-07
    - claude-sonnet-4-5-20250929
    - gemini-2.5-flash
    - gemini-2.0-flash

  # æ”¯æŒæ·±åº¦æ¨ç†/thinking çš„æ¨¡å‹
  thinking_models:
    - gpt-5-pro-2025-08-07
    - gpt-5.1
    - claude-opus-4-1-20250805
    - gemini-2.5-pro
    - deepseek-r1
    - grok-4-fast-reasoning

  # ç¼–ç¨‹èƒ½åŠ›å¼ºçš„æ¨¡å‹
  coding_specialists:
    - codestral-22b-v0.1
    - deepseek-v3.2
    - claude-sonnet-4-5-20250929
    - gpt-5.1

  # æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡çš„æ¨¡å‹
  long_context_models:
    - llama-4-scout-17b-16e-instruct  # 10M tokens
    - gemini-2.5-flash               # 1M tokens
    - claude-sonnet-4-5-20250929     # 200K tokens
```

### èƒ½åŠ›æ„ŸçŸ¥è·¯ç”±

```python
def select_model_by_capability(
    requirement: str,
    capabilities: Dict[str, List[str]],
    tier_preferences: Dict[str, List[str]],
) -> str:
    """æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹"""

    # æ£€æµ‹éœ€æ±‚
    needs_vision = "image" in requirement.lower() or "screenshot" in requirement.lower()
    needs_reasoning = any(
        kw in requirement.lower()
        for kw in ["analyze", "evaluate", "trade-off", "why"]
    )
    needs_coding = any(
        kw in requirement.lower()
        for kw in ["code", "implement", "debug", "function"]
    )
    needs_long_context = len(requirement) > 50000

    # ç­›é€‰æ»¡è¶³éœ€æ±‚çš„æ¨¡å‹
    candidates = set()

    if needs_vision:
        candidates.update(capabilities.get("multimodal_models", []))
    if needs_reasoning:
        candidates.update(capabilities.get("thinking_models", []))
    if needs_coding:
        candidates.update(capabilities.get("coding_specialists", []))
    if needs_long_context:
        candidates.update(capabilities.get("long_context_models", []))

    if not candidates:
        # é»˜è®¤è¿”å› medium é¦–é€‰
        return tier_preferences.get("medium", [])[0].split(":")[1]

    # ä»å€™é€‰ä¸­é€‰æ‹©æœ€ç»æµçš„ï¼ˆæŒ‰å±‚çº§ä»ä½åˆ°é«˜ï¼‰
    for tier in ["small", "medium", "large"]:
        for pref in tier_preferences.get(tier, []):
            model = pref.split(":")[1] if ":" in pref else pref
            if model in candidates:
                return model

    return list(candidates)[0]
```

**æ ¸å¿ƒé€»è¾‘**ï¼šèƒ½åŠ›åŒ¹é… > æˆæœ¬ä¼˜åŒ–ã€‚å…ˆç¡®ä¿æ¨¡å‹èƒ½åšè¿™ä»¶äº‹ï¼Œå†è€ƒè™‘æˆæœ¬ã€‚

---

## 30.8 é€Ÿç‡é™åˆ¶

### æŒ‰å±‚çº§å·®å¼‚åŒ–é™åˆ¶

```yaml
rate_limits:
  default_rpm: 60    # é»˜è®¤æ¯åˆ†é’Ÿè¯·æ±‚æ•°
  default_tpm: 100000  # é»˜è®¤æ¯åˆ†é’Ÿ token æ•°

  tier_overrides:
    small:
      rpm: 120        # å¿«é€Ÿæ¨¡å‹å…è®¸æ›´é«˜é¢‘ç‡
      tpm: 200000
    medium:
      rpm: 60
      tpm: 100000
    large:
      rpm: 30         # å¤æ‚æ¨¡å‹é™åˆ¶é¢‘ç‡
      tpm: 50000
```

**è®¾è®¡æ€è·¯**ï¼š
- Small æ¨¡å‹å¿«ä¸”ä¾¿å®œï¼Œå¯ä»¥æ›´æ¿€è¿›
- Large æ¨¡å‹æ…¢ä¸”è´µï¼Œé™åˆ¶é¢‘ç‡é˜²æ­¢è´¦å•å¤±æ§

### Token Bucket é™æµå™¨

```python
class RateLimiter:
    """Token bucket é™æµå™¨"""

    def __init__(self, requests_per_minute: int):
        self.requests_per_minute = requests_per_minute
        self.tokens = requests_per_minute
        self.last_refill = time.time()
        self._lock = asyncio.Lock()

    async def acquire(self):
        async with self._lock:
            now = time.time()
            elapsed = now - self.last_refill

            # è¡¥å…… tokenï¼ˆæŒ‰æ—¶é—´æµé€ï¼‰
            refill_amount = elapsed * (self.requests_per_minute / 60.0)
            self.tokens = min(self.requests_per_minute, self.tokens + refill_amount)
            self.last_refill = now

            if self.tokens >= 1:
                self.tokens -= 1
                return True

            # ç­‰å¾…è¶³å¤Ÿ token
            wait_time = (1 - self.tokens) / (self.requests_per_minute / 60.0)
            await asyncio.sleep(wait_time)
            self.tokens = 0
            return True
```

---

## 30.9 é›†ä¸­å¼å®šä»·ç®¡ç†

### å®šä»·é…ç½®

```yaml
# é›†ä¸­å¼æ¨¡å‹å®šä»·ï¼ˆUSD per 1K tokensï¼‰
# ç”¨äºæˆæœ¬è¿½è¸ªå’Œé¢„ç®—æ§åˆ¶
pricing:
  defaults:
    combined_per_1k: 0.005  # æœªçŸ¥æ¨¡å‹çš„é»˜è®¤å€¼

  models:
    openai:
      gpt-5-nano-2025-08-07:
        input_per_1k: 0.00005
        output_per_1k: 0.00040
      gpt-5-mini-2025-08-07:
        input_per_1k: 0.00025
        output_per_1k: 0.00200
      gpt-5.1:
        input_per_1k: 0.00125
        output_per_1k: 0.01000

    anthropic:
      claude-haiku-4-5-20251015:
        input_per_1k: 0.00010
        output_per_1k: 0.00050
      claude-sonnet-4-5-20250929:
        input_per_1k: 0.00030
        output_per_1k: 0.00150
      claude-opus-4-1-20250805:
        input_per_1k: 0.0150
        output_per_1k: 0.0750
```

### æˆæœ¬è¿½è¸ª

```python
def _update_usage_tracking(
    self, request: CompletionRequest, response: CompletionResponse
):
    """æ›´æ–°ä½¿ç”¨é‡è¿½è¸ª"""

    # æŒ‰ä¼šè¯è¿½è¸ª
    if request.session_id:
        if request.session_id not in self.session_usage:
            self.session_usage[request.session_id] = TokenUsage(0, 0, 0, 0.0)
        self.session_usage[request.session_id] += response.usage

    # æŒ‰ä»»åŠ¡è¿½è¸ª
    if request.task_id:
        if request.task_id not in self.task_usage:
            self.task_usage[request.task_id] = TokenUsage(0, 0, 0, 0.0)
        self.task_usage[request.task_id] += response.usage
```

### å¯è§‚æµ‹æ€§é›†æˆ

```python
# Prometheus æŒ‡æ ‡
LLM_MANAGER_COST = Counter(
    "llm_manager_cost_usd_total",
    "Accumulated cost tracked by manager (USD)",
    labelnames=("provider", "model"),
)

# æ¯æ¬¡è°ƒç”¨åè®°å½•
if _METRICS_ENABLED:
    LLM_MANAGER_COST.labels(
        response.provider, response.model
    ).inc(max(0.0, float(response.usage.estimated_cost)))
```

ç›‘æ§å±‚çº§åˆ†å¸ƒï¼š`llm_requests_total{tier="small|medium|large"}`

---

## 30.10 å¸¸è§çš„å‘

### å‘ 1ï¼šè¿‡åº¦ä¾èµ–å¤æ‚åº¦ä¼°è®¡

å¤æ‚åº¦ä¼°è®¡ä¸å‡†å¯¼è‡´æ¨¡å‹é€‰æ‹©é”™è¯¯ï¼š

```python
# é”™è¯¯ï¼šåªç”¨å¤æ‚åº¦ï¼Œä¸éªŒè¯ç»“æœ
tier = select_tier_by_complexity(query)
response = await llm.complete(query, tier=tier)

# æ­£ç¡®ï¼šåŠ å…¥éªŒè¯å’Œå‡çº§æœºåˆ¶
tier = select_tier_by_complexity(query)
response = await llm.complete(query, tier=tier)
if response.confidence < 0.7 or response.quality_score < threshold:
    # å‡çº§åˆ°æ›´å¤§æ¨¡å‹é‡è¯•
    tier = upgrade_tier(tier)
    response = await llm.complete(query, tier=tier)
```

**ç»éªŒå€¼**ï¼šå¯¹äºé‡è¦ä»»åŠ¡ï¼Œé¢„ç•™ 10-20% çš„"å‡çº§é¢„ç®—"ã€‚

### å‘ 2ï¼šå¿½ç•¥æ¨¡å‹èƒ½åŠ›å·®å¼‚

æŸäº›ä»»åŠ¡åªæœ‰ç‰¹å®šæ¨¡å‹èƒ½åšå¥½ï¼š

```python
# é”™è¯¯ï¼šåªçœ‹æˆæœ¬
tier = "small"  # æœ€ä¾¿å®œ

# æ­£ç¡®ï¼šæ£€æŸ¥èƒ½åŠ›åŒ¹é…
if has_image_input(query):
    model = select_from_multimodal_models()
elif needs_deep_reasoning(query):
    model = select_from_thinking_models()
elif is_coding_task(query):
    model = select_from_coding_specialists()
else:
    tier = select_tier_by_complexity(query)
```

### å‘ 3ï¼šç¼ºå°‘ Fallback

é¦–é€‰ provider ä¸å¯ç”¨æ—¶ä»»åŠ¡å¤±è´¥ï¼š

```python
# é”™è¯¯ï¼šåªç”¨ä¸€ä¸ª provider
response = await anthropic.complete(query)

# æ­£ç¡®ï¼šè‡ªåŠ¨ fallback
try:
    response = await manager.complete(query, tier="medium")
    # è‡ªåŠ¨æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ª provider
except AllProvidersUnavailable:
    # é™çº§åˆ°å…¶ä»–å±‚
    response = await manager.complete(query, tier="small")
```

### å‘ 4ï¼šé™æ€å¤æ‚åº¦åˆ¤æ–­

ç”¨æˆ·é—®é¢˜ç»å¸¸è¶…å‡ºé¢„æœŸï¼š

```python
# é”™è¯¯ï¼šä¸€æ¬¡æ€§åˆ¤æ–­
tier = classify_once(query)

# æ­£ç¡®ï¼šåŠ¨æ€è°ƒæ•´
tier = initial_classification(query)
response = await llm.complete(query, tier=tier)

# åŸºäºç»“æœè´¨é‡è°ƒæ•´
if needs_more_capability(response, query):
    tier = upgrade_tier(tier)
    response = await llm.complete(query, tier=tier)
```

### å‘ 5ï¼šå¿½ç•¥æç¤ºè¯ç¼“å­˜

é‡å¤æç¤ºè¯æµªè´¹é’±ï¼š

```yaml
# å¯ç”¨æç¤ºè¯ç¼“å­˜
prompt_cache:
  enabled: true
  similarity_threshold: 0.95
  ttl_seconds: 3600
  max_cache_size_mb: 2048
```

å¯¹äº System Prompt ä¸å˜çš„åœºæ™¯ï¼Œç¼“å­˜èƒ½èŠ‚çœ 50%+ çš„è¾“å…¥æˆæœ¬ã€‚

---

## 30.11 æ¡†æ¶å¯¹æ¯”

| ç‰¹æ€§ | Shannon | LangChain | LlamaIndex |
|------|---------|-----------|------------|
| å¤š Provider æ”¯æŒ | åŸç”Ÿæ”¯æŒ 9+ | é€šè¿‡é›†æˆ | é€šè¿‡é›†æˆ |
| åˆ†å±‚è·¯ç”± | é…ç½®é©±åŠ¨ | éœ€è‡ªå»º | éœ€è‡ªå»º |
| Circuit Breaker | å†…ç½® | éœ€é¢å¤–åº“ | éœ€é¢å¤–åº“ |
| æˆæœ¬è¿½è¸ª | Prometheus åŸç”Ÿ | Callbacks | Callbacks |
| èƒ½åŠ›çŸ©é˜µ | YAML é…ç½® | ä»£ç å®šä¹‰ | ä»£ç å®šä¹‰ |
| Fallback | è‡ªåŠ¨ | æ‰‹åŠ¨ | æ‰‹åŠ¨ |

---

## å›é¡¾

1. **ä¸‰å±‚æ¶æ„**ï¼šSmall (50%) / Medium (40%) / Large (10%) æ˜¯é»„é‡‘åˆ†å¸ƒ
2. **å¤æ‚åº¦è·¯ç”±**ï¼šå¯å‘å¼å¿«é€Ÿåˆ¤æ–­ + å¯é€‰æ¨¡å‹éªŒè¯
3. **èƒ½åŠ›çŸ©é˜µ**ï¼šå…ˆåŒ¹é…èƒ½åŠ›ï¼Œå†ä¼˜åŒ–æˆæœ¬
4. **éŸ§æ€§è®¾è®¡**ï¼šCircuit Breaker + è‡ªåŠ¨ Fallback
5. **å¯è§‚æµ‹æ€§**ï¼šè¿½è¸ªå±‚çº§åˆ†å¸ƒï¼Œå‘ç°æˆæœ¬å¼‚å¸¸

---

## Shannon Labï¼ˆ10 åˆ†é’Ÿä¸Šæ‰‹ï¼‰

æœ¬èŠ‚å¸®ä½ åœ¨ 10 åˆ†é’Ÿå†…æŠŠæœ¬ç« æ¦‚å¿µå¯¹åº”åˆ° Shannon æºç ã€‚

### å¿…è¯»ï¼ˆ1 ä¸ªæ–‡ä»¶ï¼‰

- `config/models.yaml`ï¼šä¸‰å±‚æ¨¡å‹é…ç½® + èƒ½åŠ›çŸ©é˜µ + å®šä»·ï¼Œæ˜¯"åˆ†å±‚ç­–ç•¥"çœŸæ­£è½åœ°çš„åœ°æ–¹

### é€‰è¯»æ·±æŒ–ï¼ˆ2 ä¸ªï¼ŒæŒ‰å…´è¶£æŒ‘ï¼‰

- `python/llm-service/llm_provider/manager.py`ï¼šLLMManager æ€ä¹ˆåšè·¯ç”±ã€ç†”æ–­ã€Fallback
- `python/llm-service/llm_service/api/complexity.py`ï¼šå¤æ‚åº¦åˆ†æ APIï¼ˆä½ ä¼šå‘ç°"åˆ¤æ–­ä»»åŠ¡éš¾åº¦"æ¯”å†™è·¯ç”±æ›´éš¾ï¼‰

---

## ç»ƒä¹ 

### ç»ƒä¹  1ï¼šå¤æ‚åº¦åˆ†æå™¨

å®ç°ä¸€ä¸ªå¤æ‚åº¦åˆ†æå™¨ï¼Œèƒ½åŒºåˆ†ä»¥ä¸‹æŸ¥è¯¢çš„å±‚çº§ï¼š
- "ä»Šå¤©å‘¨å‡ ï¼Ÿ" â†’ Small
- "æ€»ç»“è¿™ç¯‡ 2000 å­—æ–‡ç« " â†’ Medium
- "åˆ†æè¿™ä»½è´¢æŠ¥çš„é£é™©ç‚¹å¹¶ç»™å‡ºæŠ•èµ„å»ºè®®" â†’ Large

è¦æ±‚ï¼š
1. åŸºäºå…³é”®è¯å’Œé•¿åº¦çš„å¯å‘å¼è§„åˆ™
2. è¾“å‡ºå¤æ‚åº¦åˆ†æ•° (0-1) å’Œæ¨èå±‚çº§
3. å†™æµ‹è¯•ç”¨ä¾‹éªŒè¯å‡†ç¡®ç‡

### ç»ƒä¹  2ï¼šæˆæœ¬è¿½è¸ª Dashboard

åŸºäº Prometheus æŒ‡æ ‡è®¾è®¡ä¸€ä¸ªæˆæœ¬è¿½è¸ªä»ªè¡¨ç›˜ï¼š
- æŒ‰å±‚çº§æ˜¾ç¤ºè¯·æ±‚åˆ†å¸ƒ
- æŒ‰ provider æ˜¾ç¤ºæˆæœ¬è¶‹åŠ¿
- è®¾ç½®æˆæœ¬è¶…æ ‡å‘Šè­¦ (daily_budget_usd)

### ç»ƒä¹  3ï¼šåŠ¨æ€å‡çº§ç­–ç•¥

å®ç°ä¸€ä¸ª"è¯•æ¢æ€§è·¯ç”±"ç­–ç•¥ï¼š
1. é¦–æ¬¡ç”¨ Small æ¨¡å‹å°è¯•
2. å¦‚æœå“åº”è´¨é‡ä¸è¾¾æ ‡ï¼ˆå¦‚ confidence < 0.7ï¼‰ï¼Œå‡çº§åˆ° Medium
3. å¦‚æœä»ä¸è¾¾æ ‡ï¼Œå‡çº§åˆ° Large
4. è®°å½•å‡çº§æ¬¡æ•°ï¼Œç”¨äºä¼˜åŒ–åˆå§‹åˆ¤æ–­

---

## è¿›ä¸€æ­¥é˜…è¯»

- Token é¢„ç®—æ§åˆ¶ - å‚è§ç¬¬ 23 ç« 
- å¯è§‚æµ‹æ€§ä¸ç›‘æ§ - å‚è§ç¬¬ 22 ç« 
- Provider é…ç½®å®è·µ - å‚è§ Shannon config/models.yaml

---

## Part 9 æ€»ç»“

Part 9 æ¢è®¨äº† AI Agent çš„å‰æ²¿å®è·µï¼š

| ç« èŠ‚ | ä¸»é¢˜ | æ ¸å¿ƒèƒ½åŠ› |
|------|------|----------|
| Ch27 | Computer Use | è§†è§‰ç†è§£ + ç•Œé¢æ“ä½œ |
| Ch28 | Agentic Coding | ä»£ç ç”Ÿæˆ + æ²™ç®±æ‰§è¡Œ |
| Ch29 | Background Agents | Temporal è°ƒåº¦ + æŒä¹…åŒ– |
| Ch30 | åˆ†å±‚æ¨¡å‹ç­–ç•¥ | æ™ºèƒ½è·¯ç”± + æˆæœ¬ä¼˜åŒ– |

è¿™äº›èƒ½åŠ›ç»“åˆä¼ä¸šçº§åŸºç¡€è®¾æ–½ï¼ˆPart 7-8ï¼‰ï¼Œæ„æˆäº†å®Œæ•´çš„ç”Ÿäº§çº§ AI Agent å¹³å°ã€‚

ä»å•ä½“ Agent åˆ°ä¼ä¸šçº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ ¸å¿ƒæŒ‘æˆ˜ä¸å˜ï¼š**å¦‚ä½•åœ¨èƒ½åŠ›ã€æˆæœ¬ã€å¯é æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡**ã€‚åˆ†å±‚æ¨¡å‹ç­–ç•¥æ˜¯è¿™ä¸ªå¹³è¡¡çš„å…³é”®æ æ†â€”â€”ç”¨å¯¹çš„æ¨¡å‹åšå¯¹çš„äº‹ï¼Œæ—¢æ˜¯æŠ€æœ¯é—®é¢˜ï¼Œä¹Ÿæ˜¯æ¶æ„å“²å­¦ã€‚
